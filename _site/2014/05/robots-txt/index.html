

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>A dynamically-generated robots.txt: will search engine bots recognize themselves? - R. Stuart Geiger</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="R. Stuart Geiger">
<meta property="og:title" content="A dynamically-generated robots.txt: will search engine bots recognize themselves?">


  <link rel="canonical" href="https://staeiou.github.io/2014/05/robots-txt/">
  <meta property="og:url" content="https://staeiou.github.io/2014/05/robots-txt/">



  <meta property="og:description" content="In short, I built a script that dynamically generates a robots.txt file for search engine bots, who download the file when they seek direction on what parts of a website they are allowed to index. By default, it directs all bots to stay away from the entire site, but then presents an exception: only the bot that requests the robots.txt file is allowed full reign over the site. If Google’s bot downloads the robots.txt file, it will see that only Google’s bot gets to index the entire site. If Yahoo’s bot downloads the robots.txt file, it will see that only Yahoo’s bot gets to index the entire site. Of course, this is assuming that bots identify themselves to my server in a way that they recognize when it is reflected back to them.">



  <meta name="twitter:site" content="@staeiou">
  <meta name="twitter:title" content="A dynamically-generated robots.txt: will search engine bots recognize themselves?">
  <meta name="twitter:description" content="In short, I built a script that dynamically generates a robots.txt file for search engine bots, who download the file when they seek direction on what parts of a website they are allowed to index. By default, it directs all bots to stay away from the entire site, but then presents an exception: only the bot that requests the robots.txt file is allowed full reign over the site. If Google’s bot downloads the robots.txt file, it will see that only Google’s bot gets to index the entire site. If Yahoo’s bot downloads the robots.txt file, it will see that only Yahoo’s bot gets to index the entire site. Of course, this is assuming that bots identify themselves to my server in a way that they recognize when it is reflected back to them.">
  <meta name="twitter:url" content="https://staeiou.github.io/2014/05/robots-txt/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://staeiou.github.io/images/site-logo.png">
    
  

  
    <meta name="twitter:creator" content="@stuart">
  



  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2014-05-13T18:37:02-07:00">






  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Organization",
      "url": "https://staeiou.github.io",
      "logo": "https://staeiou.github.io/images/site-logo.png"
    }
  </script>



  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "R. Stuart Geiger",
      "url" : "https://staeiou.github.io",
      "sameAs" : ["https://twitter.com/mmistakes","https://facebook.com/michaelrose"]
    }
  </script>






<!-- end SEO -->


<link href="https://staeiou.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="R. Stuart Geiger Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://staeiou.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://staeiou.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://staeiou.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://staeiou.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://staeiou.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://staeiou.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://staeiou.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://staeiou.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://staeiou.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://staeiou.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://staeiou.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://staeiou.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://staeiou.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://staeiou.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://staeiou.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://staeiou.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://staeiou.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://staeiou.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->
  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://staeiou.github.io/">R. Stuart Geiger</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://staeiou.github.io/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://staeiou.github.io/articles/">Academic Articles</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://staeiou.github.io/expressions/">Expressions</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://staeiou.github.io/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://staeiou.github.io/cv/">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://staeiou.github.io/contact/">Contact</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://stuartgeiger.com/wordpress/wp-content/uploads/2008/05/oban3.jpg" alt="R. Stuart Geiger">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">R. Stuart Geiger</h3>
    <p class="author__bio">computational ethnographer & ethnographer of computation</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Berkeley, CA</li>
      
      
      
      
      
        <li><a href="https://twitter.com/staeiou"><i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/staeiou"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="A dynamically-generated robots.txt: will search engine bots recognize themselves?">
    <meta itemprop="description" content="In short, I built a script that dynamically generates a robots.txt file for search engine bots, who download the file when they seek direction on what parts of a website they are allowed to index. By default, it directs all bots to stay away from the entire site, but then presents an exception: only the bot that requests the robots.txt file is allowed full reign over the site. If Google’s bot downloads the robots.txt file, it will see that only Google’s bot gets to index the entire site. If Yahoo’s bot downloads the robots.txt file, it will see that only Yahoo’s bot gets to index the entire site. Of course, this is assuming that bots identify themselves to my server in a way that they recognize when it is reflected back to them.">
    <meta itemprop="datePublished" content="May 13, 2014">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">A dynamically-generated robots.txt: will search engine bots recognize themselves?
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  7 minute read
	
</p>
          
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2014-05-13T18:37:02-07:00">May 13, 2014</time></p>
        
        </header>
      

      <section class="page__content" itemprop="text">
        <p>In short, I built a script that dynamically generates a robots.txt file for search engine bots, who download the file when they seek direction on what parts of a website they are allowed to index. By default, it directs all bots to stay away from the entire site, but then presents an exception: only the bot that requests the robots.txt file is allowed full reign over the site. If Google’s bot downloads the robots.txt file, it will see that only Google’s bot gets to index the entire site. If Yahoo’s bot downloads the robots.txt file, it will see that only Yahoo’s bot gets to index the entire site. Of course, this is assuming that bots identify themselves to my server in a way that they recognize when it is reflected back to them.</p>

<!--more-->

<p><span style="color: #292f33;">What is a robots.txt file? Most websites have one of these very simple file called “robots.txt” on the main directory of their server. The robots.txt file has been around for almost two decades, and it is now a standardized way of communicating what pages search engine bots (or crawlers) should and should not visit. Crawlers are supposed to request and download a robots.txt file from any website they visit, and then obey the directives mentioned in such a file. Of course, there is nothing which prevents a crawler from still crawling pages which are forbidden in a robots.txt file, but most major search engine bots behave themselves. </span></p>

<p><span style="color: #292f33;">In many ways, robots.txt files stand out as a legacy from a much earlier time. When was the last time you wrote something for public distribution in a .txt file, anyway? In an age of server-side scripting and content management systems, robots.txt is also one the few public-facing files a systems administrator will actually edit and maintain by hand, manually adding and removing entries in a text editor. A robots.txt file has no changelog in it, but its revision history would be a partial chronicle of a systems administrator’s interactions with how their website is represented by various search engines.</span><span style="color: #292f33;">You can specify different directives for different bots by specifying a user agent, and well-behaved bots are supposed to look for their own user agents in a robots.txt file and follow the instructions left for them. </span>As for my own, I’m sad to report that I simply let all bots through wherever they roam, as I use a sitemap.tar.gz file which a WordPress plugin generates for me on a regular basis and submits to the major search engines. So my robots.txt file just looks like this:</p>

<pre><span style="color: #000000;">User-agent: *
</span>Allow: /</pre>

<p><span style="color: #292f33;">An interesting thing about contemporary web servers is that file formats no longer really matter as much as they used to. In fact, files don’t even have to exist as we they are typically represented in URLs. When your browser requests the page http://stuartgeiger.com/wordpress/2014/05/robots-txt, there is a directory called “wordpress” on my server, but everything after that is a fiction. There is no directory called 2014, no a subdirectory called 05, and no file called robots-txt that existed on the server before or after you downloaded it. Rather, when WordPress receives a request to download this non-existent file, it intercepts it and interprets it as a request to dynamically generate a new HTML page on the fly. WordPress queries a database for the content of the post, inserts that into a theme, and then has the server send you that HTML page — with linked images, stylesheets, and Javascript files, which often do actually exist as files on a server. The server probably stores the dynamically-generated HTML page in its memory, and sometimes there is caching to pre-generate these pages to make things faster, but other than that, the only time an HTML file of this page ever exists in any persistent form is if you save it to your hard drive. </span></p>

<p><span style="color: #292f33;">Yet robots.txt lives on, doing its job well. It doesn’t need any fancy server-side scripting; it does just fine on its own. Still, I kept thinking about what it would be like to have a script dynamically generate a robots.txt file on the fly whenever it is requested. Given that the only time a robots.txt file is usually downloaded is when an automated software agent requests it, there is something strangely poetic about an algorithmically-generated robots.txt file. It is something that would, for the most part, only ever really exist in the fleeting interaction between two automated routines. So of course I had to build one.</span></p>

<p>The code required to implement this is trivial. First, I needed to modify how my web server interprets requests, so that whenever a request was made to robots.txt, the server would execute a script called robots.php and send the client the output as robots.txt. Modify the .htaccess file to add:</p>

<pre><span style="color: #000000;">RewriteEngine On
RewriteBase /
RewriteRule ^robots.txt$ /robots.php</span></pre>

<p>Next, the PHP script itself:</p>

<pre>&lt;?php
header('Content-Type:text/plain');
echo "User-agent: *" . "\r\n";
echo "Allow: /" . "\r\n";
?&gt;
</pre>

<p>Then I realized that this was all a little impersonal, and I could do better since I’m scripting. With PHP, I can easily query the user-agent of the client which is requesting the file, the identifier it sends to the web server. Normally, user agents define the browser that is requesting the page, but bots are supposed to have an identifiable user-agent like “Googlebot” or “Twitterbot” so that you can know them when they come to visit. Instead of granting access to every user agent with the asterisk, I made it so that the user agent of the requesting client is the only one that is directed to have full access.</p>

<pre>&lt;?php
header('Content-Type:text/plain');
echo "User-agent:" . $_SERVER['HTTP_USER_AGENT'] . "\r\n";
echo "Allow: /" . "\r\n";
?&gt;</pre>

<p>After making sure this worked, I realized that I needed to go out there a little more. If the bots didn’t recognize themselves, then by default, they would still be allowed to crawl the site anyway. robots.txt works on a principle of allow by default. So I needed to add a few more lines which made it so that the robots.txt file the bot downloaded would direct all <strong>other</strong> bots to <strong>not</strong> crawl the site, but give full reign to bots with the user agent it sent the server.</p>

<pre>&lt;?php
 header('Content-Type:text/plain');
 echo "User-agent: *" . "\r\n";
 echo "Disallow: /" . "\r\n";
 echo "User-agent:" . $_SERVER['HTTP_USER_AGENT'] . "\r\n";
 echo "Allow: /" . "\r\n";
 ?&gt;</pre>

<p>This is what you get if you download it in Chrome:</p>

<pre>User-agent: *
Disallow: /
User-agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36 
Allow: /</pre>

<p>The restrictive version is now live, up at <a href="http://www.stuartgeiger.com/robots.txt">http://www.stuartgeiger.com/robots.txt</a>. I’ve also put it up <a href="https://github.com/staeiou/robots.txt.php">on github</a>, because apparently that’s what cool kids do. I’m looking forward to seeing what will happen. Google’s webmaster tools will notify me if its crawlers can’t index my site, for whatever reason, and I’m curious if Google’s bots will identify themselves to my servers in a way that they will recognize.</p>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://staeiou.github.io/tags/#algorithms" class="page__taxonomy-item" rel="tag">algorithms</a><span class="sep">, </span>
    
      
      
      <a href="https://staeiou.github.io/tags/#bots" class="page__taxonomy-item" rel="tag">bots</a><span class="sep">, </span>
    
      
      
      <a href="https://staeiou.github.io/tags/#communication" class="page__taxonomy-item" rel="tag">communication</a><span class="sep">, </span>
    
      
      
      <a href="https://staeiou.github.io/tags/#discourse" class="page__taxonomy-item" rel="tag">discourse</a><span class="sep">, </span>
    
      
      
      <a href="https://staeiou.github.io/tags/#infrastructure" class="page__taxonomy-item" rel="tag">infrastructure</a><span class="sep">, </span>
    
      
      
      <a href="https://staeiou.github.io/tags/#internet" class="page__taxonomy-item" rel="tag">internet</a><span class="sep">, </span>
    
      
      
      <a href="https://staeiou.github.io/tags/#philosophy" class="page__taxonomy-item" rel="tag">philosophy</a><span class="sep">, </span>
    
      
      
      <a href="https://staeiou.github.io/tags/#technology" class="page__taxonomy-item" rel="tag">technology</a>
    
    </span>
  </p>




  






  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://staeiou.github.io/categories/#blog-posts" class="page__taxonomy-item" rel="tag">Blog Posts</a>
    
    </span>
  </p>


      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://staeiou.github.io/2014/05/robots-txt/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://staeiou.github.io/2014/05/robots-txt/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https://staeiou.github.io/2014/05/robots-txt/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://staeiou.github.io/2014/05/robots-txt/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="https://staeiou.github.io/academic%20works/bots-bespoke-code-and-the-materiality-of-software-platforms/" class="pagination--pager" title="Bots, bespoke code, and the materiality of software platforms
">Previous</a>
    
    
      <a href="https://staeiou.github.io/2015/01/register-to-the-trace-ethnography-workshop-at-the-2015-iconference/" class="pagination--pager" title="Come to the Trace Ethnography workshop at the 2015 iConference!
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://staeiou.github.io/trace-ethnography-a-retrospective/" rel="permalink">Trace Ethnography: A Retrospective
</a>
      
    </h2>
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  8 minute read
	
</p>
    
    <p class="archive__item-excerpt" itemprop="description"><p><em>This is a cross-post of <a href="http://ethnographymatters.net/blog/2016/03/23/trace-ethnography-a-retrospective/">a post I wrote</a> for Ethnography Matters, in their <a href="http://ethnographymatters.net/editions/the-person-in-the-big-data/">“The Person in the (Big) Data” series</a></em></p>

</p>
  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://staeiou.github.io/2015/01/register-to-the-trace-ethnography-workshop-at-the-2015-iconference/" rel="permalink">Come to the Trace Ethnography workshop at the 2015 iConference!
</a>
      
    </h2>
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  1 minute read
	
</p>
    
    <p class="archive__item-excerpt" itemprop="description"><p>We’re organizing a workshop on trace ethnography at the 2015 iConference, led by Amelia Acker, Matt Burton, David Ribes, and myself. See more information about it on <a href="http://trace-ethnography.github.io/">the workshop’s website</a>, or feel free to contact me for more information.</p>

</p>
  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://staeiou.github.io/academic%20works/bots-bespoke-code-and-the-materiality-of-software-platforms/" rel="permalink">Bots, bespoke code, and the materiality of software platforms
</a>
      
    </h2>
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  1 minute read
	
</p>
    
    <p class="archive__item-excerpt" itemprop="description"><p>This is a new article published in <em>Information, Communication, and Society</em> as part of their annual special issue for the Association of Internet Researchers (AoIR) conference. This year’s special issue was edited by Lee Humphreys and Tarleton Gillespie, who did a great job throughout the whole process.</p>

</p>
  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://staeiou.github.io/academic%20works/when-the-levee-breaks-without-bots-what-happens-to-wikipedias-quality-control-processes/" rel="permalink">When the Levee Breaks: Without Bots, What Happens to Wikipedia’s Quality Control Processes?
</a>
      
    </h2>
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    
    <p class="archive__item-excerpt" itemprop="description"><p>I’ve written a number of papers about the role that automated software agents (or bots) play in Wikipedia, claiming that they are critical to the continued operation of Wikipedia. <a href="http://stuartgeiger.com/wikisym13-cluebot.pdf">This paper</a> tests this hypothesis and introduces a metric visualizing the speed at which fully-automated bots, tool-assisted cyborgs, and unassisted humans review edits in Wikipedia. In the first half of 2011, ClueBot NG – one of the most prolific counter-vandalism bots in the English-language Wikipedia – went down for four distinct periods, each period of downtime lasting from days to weeks. Aaron Halfaker and I use these periods of breakdown as naturalistic experiments to study Wikipedia’s quality control network. Our analysis showed that the overall time-to-revert damaging edits was almost doubled when this software agent was down. Yet while a significantly fewer proportion of edits made during the bot’s downtime were reverted, we found that those edits were later eventually reverted. This suggests that human agents in Wikipedia took over this quality control work, but performed it at a far slower rate.</p>

</p>
  </article>
</div>

        
      </div>
    </div>
  
</div>


    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <div align="center" style="margin: 1em 0;">
    <ins class="adsbygoogle"
         style="display:block; border-bottom: initial;"
         data-ad-client="ca-pub-7328585512091257"
         data-ad-slot="3049671934"
         data-ad-format="auto"></ins>
    </div>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="https://twitter.com/staeiou"><i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="http://github.com/staeiou"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://staeiou.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2016 R. Stuart Geiger. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="https://staeiou.github.io/assets/js/main.min.js"></script>





  </body>
</html>

