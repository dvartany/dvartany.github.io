<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="http://stuartgeiger.com/feed.xml" rel="self" type="application/atom+xml" /><link href="http://stuartgeiger.com/" rel="alternate" type="text/html" /><updated>2016-08-23T10:06:02-07:00</updated><id>http://stuartgeiger.com/</id><title>R. Stuart Geiger</title><subtitle>computational ethnographer&lt;br/&gt;ethnographer of computation</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;&quot;oban3.jpg&quot;, &quot;bio&quot;=&gt;&quot;Computational ethnographer, ethnographer of computation, and postdoc at the &lt;a style=&#39;color: black;&#39; href=&#39;http://bids.berkeley.edu&#39;&gt;Berkeley Institute for Data Science&lt;/a&gt;&quot;, &quot;location&quot;=&gt;&quot;Berkeley, CA&quot;, &quot;employer&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;&quot;https://scholar.google.com/citations?user=0AvWi3wAAAAJ&amp;hl=en&quot;, &quot;email&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;&quot;staeiou&quot;, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;&quot;http://orcid.org/0000-0001-7215-0532&quot;, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;&quot;staeiou&quot;, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;&quot;staeiou&quot;}</name></author><entry><title>Trace Ethnography: A Retrospective</title><link href="http://stuartgeiger.com/posts/2016/03/trace-ethnography-a-retrospective" rel="alternate" type="text/html" title="Trace Ethnography: A Retrospective" /><published>2016-03-28T11:55:01-07:00</published><updated>2016-03-28T11:55:01-07:00</updated><id>http://stuartgeiger.com/posts/2016/03/trace-ethnography-a-retrospective</id><content type="html" xml:base="http://stuartgeiger.com/posts/2016/03/trace-ethnography-a-retrospective">&lt;p&gt;&lt;em&gt;This is a cross-post of &lt;a href=&quot;http://ethnographymatters.net/blog/2016/03/23/trace-ethnography-a-retrospective/&quot;&gt;a post I wrote&lt;/a&gt; for Ethnography Matters, in their &lt;a href=&quot;http://ethnographymatters.net/editions/the-person-in-the-big-data/&quot;&gt;“The Person in the (Big) Data” series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When I was an M.A. student back in 2009, I was trying to explain various things about how Wikipedia worked to my then-advisor David Ribes. I had been ethnographically studying the cultures of collaboration in the encyclopedia project, and I had gotten to the point where I could look through the metadata documenting changes to Wikipedia and know quite a bit about the context of whatever activity was taking place. &lt;em&gt;I&lt;/em&gt; was able to do this because_Wikipedians_ do this: they leave publicly accessible trace data in particular ways, in order to make their actions and intentions visible to other Wikipedians. However, this was practically illegible to David, who had not done this kind of participant-observation in Wikipedia and had therefore not gained this kind of socio-technical competency.&lt;/p&gt;

&lt;p&gt;For example, if I added “” to the top an article, &lt;a href=&quot;https://en.wikipedia.org/wiki/Template:Db-a7&quot;&gt;a big red notice&lt;/a&gt; would be automatically added to the page, saying that the page has been nominated for “&lt;a href=&quot;http://enwp.org/WP:CSD&quot;&gt;speedy deletion&lt;/a&gt;.” Tagging the article in this way would also put it into various information flows where Wikipedia administrators would review it. If any of Wikipedia’s administrators agreed that the article met speedy deletion criteria A7, then they would be empowered to unilaterally delete it without further discussion. If I was not the article’s creator, I could remove the  trace from the article to take it out of the speedy deletion process, which means the person who nominated it for deletion would have to go through the standard deletion process. However, if I was the article’s creator, it would not be proper for me to remove that tag — and if I did, others would find out and put it back. If someone added the “” trace to an article I created, I could add “” below it in order to inhibit this process a bit — although a hangon is a just a request, it does not prevent an administrator from deleting the article.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot; style=&quot;width: 755px; border: 0;&quot;&gt;
  &lt;p&gt;
    &lt;img class=&quot;aligncenter&quot; src=&quot;http://i2.wp.com/upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Wiki_Women%27s_Edit-a-thon-1.jpg/800px-Wiki_Women%27s_Edit-a-thon-1.jpg?resize=680%2C453&amp;amp;ssl=1&quot; alt=&quot;File:Wiki Women&#39;s Edit-a-thon-1.jpg&quot; width=&quot;745&quot; height=&quot;496&quot; /&gt;
  &lt;/p&gt;
  
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    &lt;em&gt;Wikipedians at an in-person edit-a-thon (the Women’s History Month edit-a-thon in 2012). However, most of the time, Wikipedians don’t get to do their work sitting right next to each other, which is why they rely extensively on trace data to coordinate render their activities accountable to each other. Photo by &lt;a href=&quot;https://en.wikipedia.org/wiki/File:Wiki_Women%27s_Edit-a-thon-1.jpg&quot;&gt;Matthew Roth, CC-BY-SA 3.0&lt;/a&gt;&lt;/em&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;I knew all of this both because Wikipedians told me and because this was something I experienced again and again as a participant observer. Wikipedians had documented this documentary practice in many different places on Wikipedia’s meta pages. I had first-hand experience with these trace data, first on the receiving end with one of my own articles. Then later, I became someone who nominated others’ articles for deletion. When I was learning how to participate in the project as a Wikipedian (which I now consider myself to be), I started to use these kinds of trace data practices and conventions to signify my own actions and intentions to others. This made things far easier for me as a Wikipedian, in the same way that learning my university’s arcane budgeting and human resource codes helps me navigate that bureaucracy far easier.&lt;span id=&quot;more-10287&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This “trace ethnography” emerged out of a realization that people in mediated communities and organizations increasingly rely on these kinds of techniques to render their own activities and intentions legible to each other. I should note that this was not my and David’s original insight — it is one that can can be found across the fields of history, communication studies, micro-sociology, ethnomethodology, organizational studies, science and technology studies, computer-supported cooperative work, and more. As we say in the paper, we merely “assemble their various solutions” to the problem of how to qualitatively study interaction at scale and at a distance. There are jargons, conventions, and grammars learned as a condition of membership in any group, and people learn how to interact with others by learning these techniques.&lt;/p&gt;

&lt;p&gt;The affordances of mediated platforms are increasingly being used by participants themselves to manage collaboration and context at massive scales and asynchronous latencies. Part of the trace ethnography approach involves coming to understand why these kinds of systems were developed in the way that they were. For me and Wikipedia’s deletion process, it went from being strange and obtuse to something that I expected and anticipated. I got frustrated when newcomers didn’t have the proper literacy to communicate their intentions in a way that I and other Wikipedians would understand. I am now at the point where I can even morally defend this trace-based process as Wikipedians do. I can list reason after reason why this particular process ought to unfold in the way that it does, independent of my own views on this process. I understand the values that are embedded in and assumed by this process, and they cohere with other values I have found among Wikipedians. And I’ve also met Wikipedians who are massive critics of this process and think that we should be using a far different way to deal with inappropriate articles. I’ve even helped redesign it a bit.&lt;/p&gt;

&lt;p&gt;Trace ethnography is based in the realization that these practices around metadata are learned literacies and constitute a crucial part of what it means to participate in many communities and organizations. It turns our attention to an ethnographic understanding of these practices as they make sense for the people who rely on them. In this approach, reading through log data can be seen as a form of participation, not just observation — if and only if this is how members themselves spend their time. However, it is crucial that this approach is distinguished from more passive forms of ethnography (such as “lurker ethnography”), as trace ethnography involves an ethnographer’s socialization into a group prior to the ability to decode and interpret trace data. If trace data is simply being automatically generated without it being integrated into people’s practices of participation, if people in a community don’t regularly rely on following traces in their everyday practices, then the “ethnography” label is likely not appropriate.&lt;/p&gt;

&lt;p&gt;Looking at all kinds of online communities and mediated organizations, Wikipedia’s deletion process might appear to be the most arcane and out-of-the-ordinary. However, modes of participation are increasingly linked to the encoding and decoding of trace data, whether that is a global scientific collaboration, an open source software project, a guild of role playing gamers, an activist network, a news organization, a governmental agency, and so on. Computer programmers frequently rely on GitHub to collaborate, and they have their own ways of using things like issues, commit comments, and pull requests to interact with each other. Without being on GitHub, it’s hard for an ethnographer who studies software development to be a fully-immersed participant-observer, because they would be missing a substantial amount of activity — even if they are constantly in the same room as the programmers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;More about trace ethnography&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to read more about “trace ethnography,” we first used this term in “&lt;a href=&quot;http://www.stuartgeiger.com/papers/cscw-sustaining-order-wikipedia.pdf&quot;&gt;The Work of Sustaining Order in Wikipedia: The Banning of a Vandal&lt;/a&gt;,” which I co-authored with my then-advisor David Ribes in the proceedings of the CSCW 2010 conference. We then wrote &lt;a href=&quot;http://www.stuartgeiger.com/trace-ethnography-hicss-geiger-ribes.pdf&quot;&gt;a followup paper in the proceedings of HICSS 2011&lt;/a&gt; to give a more general introduction to this method, in which we ‘inverted’ the CSCW 2011 paper, explaining more of the methods we used. We also held a workshop at the 2015 iConference with Amelia Acker and Matt Burton — the details of that workshop (and the collaborative notes) can be found at&lt;a href=&quot;http://trace-ethnography.github.io/&quot;&gt;http://trace-ethnography.github.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some examples of projects employing this method:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ford, H. and Geiger, R.S. “Writing up rather than writing down: Becoming Wikipedia literate.” &lt;em&gt;Proceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration&lt;/em&gt;. ACM, 2012. &lt;a href=&quot;http://www.stuartgeiger.com/writing-up-wikisym.pdf&quot;&gt;http://www.stuartgeiger.com/writing-up-wikisym.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ribes, D., Jackson, S., Geiger, R.S., Burton, M., &amp;amp; Finholt, T. (2013). Artifacts that organize: Delegation in the distributed organization. &lt;em&gt;Information and Organization&lt;/em&gt;, &lt;em&gt;23&lt;/em&gt;(1), 1-14. &lt;a href=&quot;http://www.stuartgeiger.com/artifacts-that-organize.pdf&quot;&gt;http://www.stuartgeiger.com/artifacts-that-organize.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Mugar, G., Østerlund, C., Hassman, K. D., Crowston, K., &amp;amp; Jackson, C. B. (2014). Planet hunters and seafloor explorers: legitimate peripheral participation through practice proxies in online citizen science. In_Proceedings of the 17th ACM conference on Computer supported cooperative work &amp;amp; social computing_ (pp. 109-119). ACM. &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2531721&quot;&gt;http://dl.acm.org/citation.cfm?id=2531721&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Howison, J., &amp;amp; Crowston, K. (2014). Collaboration Through Open Superposition: A Theory of the Open Source Way. &lt;em&gt;Mis Quarterly&lt;/em&gt;, &lt;em&gt;38&lt;/em&gt;(1), 29-50. &lt;a href=&quot;http://aisel.aisnet.org/cgi/viewcontent.cgi?article=3156&amp;amp;context=misq&quot;&gt;http://aisel.aisnet.org/cgi/viewcontent.cgi?article=3156&amp;amp;context=misq&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Burton, M. (2015). Blogs as Infrastructure for Scholarly Communication. Doctoral Dissertation, University of Michigan.&lt;a href=&quot;http://deepblue.lib.umich.edu/bitstream/handle/2027.42/111592/mcburton_1.pdf&quot;&gt;http://deepblue.lib.umich.edu/bitstream/handle/2027.42/111592/mcburton_1.pdf&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;&quot;oban3.jpg&quot;, &quot;bio&quot;=&gt;&quot;Computational ethnographer, ethnographer of computation, and postdoc at the &lt;a style=&#39;color: black;&#39; href=&#39;http://bids.berkeley.edu&#39;&gt;Berkeley Institute for Data Science&lt;/a&gt;&quot;, &quot;location&quot;=&gt;&quot;Berkeley, CA&quot;, &quot;employer&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;&quot;https://scholar.google.com/citations?user=0AvWi3wAAAAJ&amp;hl=en&quot;, &quot;email&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;&quot;staeiou&quot;, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;&quot;http://orcid.org/0000-0001-7215-0532&quot;, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;&quot;staeiou&quot;, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;&quot;staeiou&quot;}</name></author><summary>This is a cross-post of a post I wrote for Ethnography Matters, in their “The Person in the (Big) Data” series</summary></entry><entry><title>Come to the Trace Ethnography workshop at the 2015 iConference!</title><link href="http://stuartgeiger.com/posts/2015/01/register-to-the-trace-ethnography-workshop-at-the-2015-iconference/" rel="alternate" type="text/html" title="Come to the Trace Ethnography workshop at the 2015 iConference!" /><published>2015-01-08T08:23:09-08:00</published><updated>2015-01-08T08:23:09-08:00</updated><id>http://stuartgeiger.com/posts/2015/01/register-to-the-trace-ethnography-workshop-at-the-2015-iconference</id><content type="html" xml:base="http://stuartgeiger.com/posts/2015/01/register-to-the-trace-ethnography-workshop-at-the-2015-iconference/">&lt;p&gt;We’re organizing a workshop on trace ethnography at the 2015 iConference, led by Amelia Acker, Matt Burton, David Ribes, and myself. See more information about it on &lt;a href=&quot;http://trace-ethnography.github.io/&quot;&gt;the workshop’s website&lt;/a&gt;, or feel free to contact me for more information.&lt;/p&gt;

&lt;p&gt;Date: March 24th 2015, 9:00-a.m.-5:00 p.m.&lt;/p&gt;

&lt;p&gt;Location: iConference venue, Newport Beach Marriott Hotel &amp;amp; Spa, Newport Beach, CA&lt;/p&gt;

&lt;p&gt;Deadline to register &lt;a style=&quot;font-weight: inherit; font-style: inherit; color: #007edf;&quot; href=&quot;http://goo.gl/forms/Cc4G1ULyXv&quot;&gt;through this form&lt;/a&gt;: Feb 1st, 2015. Note: you will also have to register through the &lt;a style=&quot;font-weight: inherit; font-style: inherit; color: #007edf;&quot; href=&quot;http://ischools.org/the-iconference/&quot;&gt;official iConference website&lt;/a&gt; as well.&lt;/p&gt;

&lt;p&gt;Notification: Feb 15th, 2015&lt;/p&gt;

&lt;p&gt;Description: This workshop introduces participants to trace ethnography, building a network of scholars interested in the collection and interpretation of trace data and distributed documentary practices. The intended audience is broad, and participants need not have any existing experience working with trace data from either qualitative or quantitative approaches. The workshop provides an interactive introduction to the background, theories, methods, and applications–present and future–of trace ethnography. Participants with more experience in this area will demonstrate how they apply these techniques in their own research, discussing various issues as they arise. The workshop is intended to help researchers identify documentary traces, plan for their collection and analysis, and further formulate trace ethnography as it is currently conceived. In all, this workshop will support the advancement of boundaries, theories, concepts, and applications in trace ethnography, identifying the diversity of approaches that can be assembled around the idea of ‘trace ethnography’ within the iSchool community.&lt;/p&gt;</content><author><name>stuart</name></author><category term="community" /><category term="conference" /><category term="ethnography" /><category term="qualitative" /><category term="research" /><category term="trace data" /><category term="virtual" /><summary>We’re organizing a workshop on trace ethnography at the 2015 iConference, led by Amelia Acker, Matt Burton, David Ribes, and myself. See more information about it on the workshop’s website, or feel free to contact me for more information.</summary></entry><entry><title>A dynamically-generated robots.txt: will search engine bots recognize themselves?</title><link href="http://stuartgeiger.com/posts/2014/05/robots-txt/" rel="alternate" type="text/html" title="A dynamically-generated robots.txt: will search engine bots recognize themselves?" /><published>2014-05-13T18:37:02-07:00</published><updated>2014-05-13T18:37:02-07:00</updated><id>http://stuartgeiger.com/posts/2014/05/robots-txt</id><content type="html" xml:base="http://stuartgeiger.com/posts/2014/05/robots-txt/">&lt;p&gt;In short, I built a script that dynamically generates a robots.txt file for search engine bots, who download the file when they seek direction on what parts of a website they are allowed to index. By default, it directs all bots to stay away from the entire site, but then presents an exception: only the bot that requests the robots.txt file is allowed full reign over the site. If Google’s bot downloads the robots.txt file, it will see that only Google’s bot gets to index the entire site. If Yahoo’s bot downloads the robots.txt file, it will see that only Yahoo’s bot gets to index the entire site. Of course, this is assuming that bots identify themselves to my server in a way that they recognize when it is reflected back to them.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;span style=&quot;color: #292f33;&quot;&gt;What is a robots.txt file? Most websites have one of these very simple file called “robots.txt” on the main directory of their server. The robots.txt file has been around for almost two decades, and it is now a standardized way of communicating what pages search engine bots (or crawlers) should and should not visit. Crawlers are supposed to request and download a robots.txt file from any website they visit, and then obey the directives mentioned in such a file. Of course, there is nothing which prevents a crawler from still crawling pages which are forbidden in a robots.txt file, but most major search engine bots behave themselves. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #292f33;&quot;&gt;In many ways, robots.txt files stand out as a legacy from a much earlier time. When was the last time you wrote something for public distribution in a .txt file, anyway? In an age of server-side scripting and content management systems, robots.txt is also one the few public-facing files a systems administrator will actually edit and maintain by hand, manually adding and removing entries in a text editor. A robots.txt file has no changelog in it, but its revision history would be a partial chronicle of a systems administrator’s interactions with how their website is represented by various search engines.&lt;/span&gt;&lt;span style=&quot;color: #292f33;&quot;&gt;You can specify different directives for different bots by specifying a user agent, and well-behaved bots are supposed to look for their own user agents in a robots.txt file and follow the instructions left for them. &lt;/span&gt;As for my own, I’m sad to report that I simply let all bots through wherever they roam, as I use a sitemap.tar.gz file which a WordPress plugin generates for me on a regular basis and submits to the major search engines. So my robots.txt file just looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;span style=&quot;color: #000000;&quot;&gt;User-agent: *
&lt;/span&gt;Allow: /&lt;/pre&gt;

&lt;p&gt;&lt;span style=&quot;color: #292f33;&quot;&gt;An interesting thing about contemporary web servers is that file formats no longer really matter as much as they used to. In fact, files don’t even have to exist as we they are typically represented in URLs. When your browser requests the page http://stuartgeiger.com/wordpress/2014/05/robots-txt, there is a directory called “wordpress” on my server, but everything after that is a fiction. There is no directory called 2014, no a subdirectory called 05, and no file called robots-txt that existed on the server before or after you downloaded it. Rather, when WordPress receives a request to download this non-existent file, it intercepts it and interprets it as a request to dynamically generate a new HTML page on the fly. WordPress queries a database for the content of the post, inserts that into a theme, and then has the server send you that HTML page — with linked images, stylesheets, and Javascript files, which often do actually exist as files on a server. The server probably stores the dynamically-generated HTML page in its memory, and sometimes there is caching to pre-generate these pages to make things faster, but other than that, the only time an HTML file of this page ever exists in any persistent form is if you save it to your hard drive. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #292f33;&quot;&gt;Yet robots.txt lives on, doing its job well. It doesn’t need any fancy server-side scripting; it does just fine on its own. Still, I kept thinking about what it would be like to have a script dynamically generate a robots.txt file on the fly whenever it is requested. Given that the only time a robots.txt file is usually downloaded is when an automated software agent requests it, there is something strangely poetic about an algorithmically-generated robots.txt file. It is something that would, for the most part, only ever really exist in the fleeting interaction between two automated routines. So of course I had to build one.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The code required to implement this is trivial. First, I needed to modify how my web server interprets requests, so that whenever a request was made to robots.txt, the server would execute a script called robots.php and send the client the output as robots.txt. Modify the .htaccess file to add:&lt;/p&gt;

&lt;pre&gt;&lt;span style=&quot;color: #000000;&quot;&gt;RewriteEngine On
RewriteBase /
RewriteRule ^robots.txt$ /robots.php&lt;/span&gt;&lt;/pre&gt;

&lt;p&gt;Next, the PHP script itself:&lt;/p&gt;

&lt;pre&gt;&amp;lt;?php
header(&#39;Content-Type:text/plain&#39;);
echo &quot;User-agent: *&quot; . &quot;\r\n&quot;;
echo &quot;Allow: /&quot; . &quot;\r\n&quot;;
?&amp;gt;
&lt;/pre&gt;

&lt;p&gt;Then I realized that this was all a little impersonal, and I could do better since I’m scripting. With PHP, I can easily query the user-agent of the client which is requesting the file, the identifier it sends to the web server. Normally, user agents define the browser that is requesting the page, but bots are supposed to have an identifiable user-agent like “Googlebot” or “Twitterbot” so that you can know them when they come to visit. Instead of granting access to every user agent with the asterisk, I made it so that the user agent of the requesting client is the only one that is directed to have full access.&lt;/p&gt;

&lt;pre&gt;&amp;lt;?php
header(&#39;Content-Type:text/plain&#39;);
echo &quot;User-agent:&quot; . $_SERVER[&#39;HTTP_USER_AGENT&#39;] . &quot;\r\n&quot;;
echo &quot;Allow: /&quot; . &quot;\r\n&quot;;
?&amp;gt;&lt;/pre&gt;

&lt;p&gt;After making sure this worked, I realized that I needed to go out there a little more. If the bots didn’t recognize themselves, then by default, they would still be allowed to crawl the site anyway. robots.txt works on a principle of allow by default. So I needed to add a few more lines which made it so that the robots.txt file the bot downloaded would direct all &lt;strong&gt;other&lt;/strong&gt; bots to &lt;strong&gt;not&lt;/strong&gt; crawl the site, but give full reign to bots with the user agent it sent the server.&lt;/p&gt;

&lt;pre&gt;&amp;lt;?php
 header(&#39;Content-Type:text/plain&#39;);
 echo &quot;User-agent: *&quot; . &quot;\r\n&quot;;
 echo &quot;Disallow: /&quot; . &quot;\r\n&quot;;
 echo &quot;User-agent:&quot; . $_SERVER[&#39;HTTP_USER_AGENT&#39;] . &quot;\r\n&quot;;
 echo &quot;Allow: /&quot; . &quot;\r\n&quot;;
 ?&amp;gt;&lt;/pre&gt;

&lt;p&gt;This is what you get if you download it in Chrome:&lt;/p&gt;

&lt;pre&gt;User-agent: *
Disallow: /
User-agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36 
Allow: /&lt;/pre&gt;

&lt;p&gt;The restrictive version is now live, up at &lt;a href=&quot;http://www.stuartgeiger.com/robots.txt&quot;&gt;http://www.stuartgeiger.com/robots.txt&lt;/a&gt;. I’ve also put it up &lt;a href=&quot;https://github.com/staeiou/robots.txt.php&quot;&gt;on github&lt;/a&gt;, because apparently that’s what cool kids do. I’m looking forward to seeing what will happen. Google’s webmaster tools will notify me if its crawlers can’t index my site, for whatever reason, and I’m curious if Google’s bots will identify themselves to my servers in a way that they will recognize.&lt;/p&gt;</content><author><name>stuart</name></author><category term="algorithms" /><category term="bots" /><category term="communication" /><category term="discourse" /><category term="infrastructure" /><category term="internet" /><category term="philosophy" /><category term="technology" /><summary>In short, I built a script that dynamically generates a robots.txt file for search engine bots, who download the file when they seek direction on what parts of a website they are allowed to index. By default, it directs all bots to stay away from the entire site, but then presents an exception: only the bot that requests the robots.txt file is allowed full reign over the site. If Google’s bot downloads the robots.txt file, it will see that only Google’s bot gets to index the entire site. If Yahoo’s bot downloads the robots.txt file, it will see that only Yahoo’s bot gets to index the entire site. Of course, this is assuming that bots identify themselves to my server in a way that they recognize when it is reflected back to them.</summary></entry><entry><title>Bots, bespoke code, and the materiality of software platforms</title><link href="http://stuartgeiger.com/posts/2014/01/bots-bespoke-code-and-the-materiality-of-software-platforms/" rel="alternate" type="text/html" title="Bots, bespoke code, and the materiality of software platforms" /><published>2014-01-06T13:22:54-08:00</published><updated>2014-01-06T13:22:54-08:00</updated><id>http://stuartgeiger.com/posts/2014/01/bots-bespoke-code-and-the-materiality-of-software-platforms</id><content type="html" xml:base="http://stuartgeiger.com/posts/2014/01/bots-bespoke-code-and-the-materiality-of-software-platforms/">&lt;p&gt;This is a new article published in &lt;em&gt;Information, Communication, and Society&lt;/em&gt; as part of their annual special issue for the Association of Internet Researchers (AoIR) conference. This year’s special issue was edited by Lee Humphreys and Tarleton Gillespie, who did a great job throughout the whole process.&lt;/p&gt;

&lt;p&gt;Abstract: This article introduces and discusses the role of &lt;em&gt;bespoke code&lt;/em&gt; in Wikipedia, which is code that runs alongside a platform or system, rather than being integrated into server-side codebases by individuals with privileged access to the server. Bespoke code complicates the common metaphors of platforms and sovereignty that we typically use to discuss the governance and regulation of software systems through code. Specifically, the work of automated software agents (bots) in the operation and administration of Wikipedia is examined, with a focus on the materiality of code. As bots extend and modify the functionality of sites like Wikipedia, but must be continuously operated on computers that are independent from the servers hosting the site, they involve alternative relations of power and code. Instead of taking for granted the pre-existing stability of Wikipedia as a platform, bots and other bespoke code require that we examine not only the software code itself, but also the concrete, historically contingent material conditions under which this code is run. To this end, this article weaves a series of autobiographical vignettes about the author’s experiences as a bot developer alongside more traditional academic discourse.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.tandfonline.com/doi/full/10.1080/1369118X.2013.873069&quot; target=&quot;_blank&quot;&gt;Official version at Information, Communication, and Society&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://stuartgeiger.com/bespoke-code-ics.pdf&quot; target=&quot;_blank&quot;&gt;Author’s post-print, free download [PDF, 382kb]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</content><author><name>stuart</name></author><category term="bots" /><category term="discourse" /><category term="governance" /><category term="infrastructure" /><category term="internet" /><category term="power" /><category term="software" /><category term="technology" /><category term="wikipedia" /><summary>This is a new article published in Information, Communication, and Society as part of their annual special issue for the Association of Internet Researchers (AoIR) conference. This year’s special issue was edited by Lee Humphreys and Tarleton Gillespie, who did a great job throughout the whole process.</summary></entry><entry><title>When the Levee Breaks: Without Bots, What Happens to Wikipedia’s Quality Control Processes?</title><link href="http://stuartgeiger.com/posts/2013/08/when-the-levee-breaks-without-bots-what-happens-to-wikipedias-quality-control-processes" rel="alternate" type="text/html" title="When the Levee Breaks: Without Bots, What Happens to Wikipedia’s Quality Control Processes?" /><published>2013-08-16T20:49:42-07:00</published><updated>2013-08-16T20:49:42-07:00</updated><id>http://stuartgeiger.com/posts/2013/08/when-the-levee-breaks-without-bots-what-happens-to-wikipedias-quality-control-processes</id><content type="html" xml:base="http://stuartgeiger.com/posts/2013/08/when-the-levee-breaks-without-bots-what-happens-to-wikipedias-quality-control-processes">&lt;p&gt;I’ve written a number of papers about the role that automated software agents (or bots) play in Wikipedia, claiming that they are critical to the continued operation of Wikipedia. &lt;a href=&quot;http://stuartgeiger.com/wikisym13-cluebot.pdf&quot;&gt;This paper&lt;/a&gt; tests this hypothesis and introduces a metric visualizing the speed at which fully-automated bots, tool-assisted cyborgs, and unassisted humans review edits in Wikipedia. In the first half of 2011, ClueBot NG – one of the most prolific counter-vandalism bots in the English-language Wikipedia – went down for four distinct periods, each period of downtime lasting from days to weeks. Aaron Halfaker and I use these periods of breakdown as naturalistic experiments to study Wikipedia’s quality control network. Our analysis showed that the overall time-to-revert damaging edits was almost doubled when this software agent was down. Yet while a significantly fewer proportion of edits made during the bot’s downtime were reverted, we found that those edits were later eventually reverted. This suggests that human agents in Wikipedia took over this quality control work, but performed it at a far slower rate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://stuartgeiger.com/wordpress/2013/09/when-the-levee-breaks-without-bots-what-happens-to-wikipedias-quality-control-processes/revert/&quot; rel=&quot;attachment wp-att-535&quot;&gt;&lt;img class=&quot;size-full wp-image-535 aligncenter&quot; src=&quot;http://stuartgeiger.com/wordpress/wp-content/uploads/2013/09/revert.png&quot; alt=&quot;revert&quot; width=&quot;480&quot; height=&quot;524&quot; srcset=&quot;http://stuartgeiger.com/wordpress/wp-content/uploads/2013/09/revert.png 480w, http://stuartgeiger.com/wordpress/wp-content/uploads/2013/09/revert-274x300.png 274w&quot; sizes=&quot;(max-width: 480px) 100vw, 480px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>stuart</name></author><category term="bots" /><category term="governance" /><category term="infrastructure" /><category term="quantitative" /><category term="software" /><category term="technology" /><category term="wikipedia" /><summary>I’ve written a number of papers about the role that automated software agents (or bots) play in Wikipedia, claiming that they are critical to the continued operation of Wikipedia. This paper tests this hypothesis and introduces a metric visualizing the speed at which fully-automated bots, tool-assisted cyborgs, and unassisted humans review edits in Wikipedia. In the first half of 2011, ClueBot NG – one of the most prolific counter-vandalism bots in the English-language Wikipedia – went down for four distinct periods, each period of downtime lasting from days to weeks. Aaron Halfaker and I use these periods of breakdown as naturalistic experiments to study Wikipedia’s quality control network. Our analysis showed that the overall time-to-revert damaging edits was almost doubled when this software agent was down. Yet while a significantly fewer proportion of edits made during the bot’s downtime were reverted, we found that those edits were later eventually reverted. This suggests that human agents in Wikipedia took over this quality control work, but performed it at a far slower rate.</summary></entry><entry><title>About a bot: reflections on building software agents</title><link href="http://stuartgeiger.com/posts/2013/08/about-a-bot-reflections-on-building-software-agents/" rel="alternate" type="text/html" title="About a bot: reflections on building software agents" /><published>2013-08-12T21:22:22-07:00</published><updated>2013-08-12T21:22:22-07:00</updated><id>http://stuartgeiger.com/posts/2013/08/about-a-bot-reflections-on-building-software-agents</id><content type="html" xml:base="http://stuartgeiger.com/posts/2013/08/about-a-bot-reflections-on-building-software-agents/">&lt;p&gt;&lt;a href=&quot;http://ethnographymatters.net/2013/08/13/about-a-bot/&quot;&gt;This post for Ethnography Matters&lt;/a&gt; is a very personal, reflective musing about the first bot I ever developed for Wikipedia. It makes the argument that while it is certainly important to think about software code and algorithms behind bots and other AI agents, they are not immaterial. In fact, the physical locations and social contexts in which they are run are often critical to understanding how they both ‘live’ and ‘die’.&lt;/p&gt;</content><author><name>stuart</name></author><category term="bots" /><category term="ethnography" /><category term="infrastructure" /><category term="materiality" /><category term="philosophy" /><category term="software" /><category term="technology" /><category term="wikipedia" /><summary>This post for Ethnography Matters is a very personal, reflective musing about the first bot I ever developed for Wikipedia. It makes the argument that while it is certainly important to think about software code and algorithms behind bots and other AI agents, they are not immaterial. In fact, the physical locations and social contexts in which they are run are often critical to understanding how they both ‘live’ and ‘die’.</summary></entry><entry><title>Bots and Cyborgs: Wikipedia’s Immune System</title><link href="http://stuartgeiger.com/posts/2012/10/bots-and-cyborgs-wikipedias-immune-system/" rel="alternate" type="text/html" title="Bots and Cyborgs: Wikipedia&amp;#8217;s Immune System" /><published>2012-10-16T21:13:31-07:00</published><updated>2012-10-16T21:13:31-07:00</updated><id>http://stuartgeiger.com/posts/2012/10/bots-and-cyborgs-wikipedias-immune-system</id><content type="html" xml:base="http://stuartgeiger.com/posts/2012/10/bots-and-cyborgs-wikipedias-immune-system/">&lt;p&gt;My frequent collaborator Aaron Halfaker has written up a &lt;a href=&quot;http://stuartgeiger.com/bots-cyborgs-halfaker.pdf&quot; target=&quot;_blank&quot;&gt;fantastic article&lt;/a&gt; with John Riedl in &lt;em&gt;Computer&lt;/em&gt; reviewing a lot of the work we’ve done on algorithmic agents in Wikipedia, casting them as Wikipedia’s immune system. Choice quote:  “These bots and cyborgs are more than tools to better manage content quality on Wikipedia—through their interaction with humans, they’re fundamentally changing its culture.”&lt;/p&gt;</content><author><name>stuart</name></author><category term="bots" /><category term="governance" /><category term="information" /><category term="infrastructure" /><category term="power" /><category term="wikipedia" /><summary>My frequent collaborator Aaron Halfaker has written up a fantastic article with John Riedl in Computer reviewing a lot of the work we’ve done on algorithmic agents in Wikipedia, casting them as Wikipedia’s immune system. Choice quote:  “These bots and cyborgs are more than tools to better manage content quality on Wikipedia—through their interaction with humans, they’re fundamentally changing its culture.”</summary></entry><entry><title>An apologia for instagram photos of pumpkin spice lattes and other serious things</title><link href="http://stuartgeiger.com/posts/2012/09/on-instagram-photos-of-pumpkin-spice-lattes-and-other-serious-things/" rel="alternate" type="text/html" title="An apologia for instagram photos of pumpkin spice lattes and other serious things" /><published>2012-09-09T14:10:09-07:00</published><updated>2012-09-09T14:10:09-07:00</updated><id>http://stuartgeiger.com/posts/2012/09/on-instagram-photos-of-pumpkin-spice-lattes-and-other-serious-things</id><content type="html" xml:base="http://stuartgeiger.com/posts/2012/09/on-instagram-photos-of-pumpkin-spice-lattes-and-other-serious-things/">&lt;p&gt;I don’t normally pick on people whose work I really admire, but I recently saw &lt;a href=&quot;https://twitter.com/samplereality/status/244151842974609408&quot;&gt;a tweet&lt;/a&gt; from Mark Sample that struck a nerve: “Look, if you don’t instagram your first pumpkin spice latte of the season, humanity’s historical record will be dangerously impoverished.”  While it got quite a number of retweets and equally snarky responses, he is far from the first to make such a flippant critique of the vapid nature of social media.  It also seriously upset me for reasons that I’ve been trying to work out, which is why I found myself doing one of those shifts that researchers of knowledge production tend to do far too often with critics: don’t get mad, get reflexive.  What is it that makes such a sentiment resonate with us, particularly when it is issued over Twitter, a platform that is the target of this kind of critique?  The reasons have to do with a fundamental disagreement over what it means to interact in a mediated space: do we understand our posts, status updates, and shared photos as representations of how we exist in the world which collectively constitute a certain persistent performance of the self, or do we understand them a form of communication in which we subjectively and interactionally relate our experience of the world to others?&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;This comment also got me thinking because it reminded me of an interaction I had at Media in Transition 6, the first major conference I attended as a presenter.  The year’s theme was “storage and transmission,” and there were a lot of well-established scholars from a variety of fields talking about social media in terms of archives and memory practices.  I remember one discussion where people were talking about how exciting it was see the widespread emergence of Facebook photo albums, arguing that youth who share photos on Facebook were engaging in the 21st century equivalent of scrapbooking – a once-common cultural practice which had been in serious decline.  I raised my hand and made a comment I’m not sure was fully grasped: that as one of the youngest people in the room, my friends and I understood photo sharing not a form of archiving but a mode of communication.  In other words, I take a photo of the MIT Media Lab and share it on Facebook primarily to tell my friends that I’m in Boston at a conference.  Sure, there is archival value to this kind of activity, but that is an added benefit which we occasionally utilize – and always after the fact.  We don’t take a picture to remember an event and then later remember that event.  We take a picture to communicate an event, later remembering a strange hybrid of the event itself and all the interactions we had about the event.  This is especially the case with something like Facebook’s timeline: instead of carefully assembling scrapbooks ourselves, we have delegated these memory practices to Facebook’s algorithms.&lt;/p&gt;

&lt;p&gt;Returning to instagram photos of pumpkin spice lattes, I admit that as a twentysomething techie-hipster in the Bay Area, I use not just Twitter, but instagram, Tumblr, and a variety of other social media platforms.  I also enjoy pumpkin spice lattes, perhaps because they are delicious, but also because I really do take in all those little things that tell me that summer is ending and autumn will soon begin.  We don’t have that much seasonal variation in the Bay Area, and coffee is a big deal here as it is everywhere – it is the world’s most popular drug.  All this to say that the first advertisement for pumpkin spice lattes plastered on the side of a Starbucks is something I notice.  And so I take photos of them, which I share with my friends and strangers.  Some of them are in the Bay Area and have the same seasonal cues I do, while some are in completely different parts of the world, where frozen water falls from the sky and other crazy things like that.  Together, we engage not so much in an act of collective sensemaking, but the sharing of a common experience: thanks to this and a hundred other little reminders, we know that winter is coming.&lt;/p&gt;

&lt;p&gt;I don’t do it because I think I’m contributing to some grand archive of humanity’s historical record.  Not even close.  In fact, if that is how I thought about most of my social media practices, I would be so anxious about choosing what to post and when that I wouldn’t make use of it at all.  I know this because there was a time when I did think of my social media usage in such a way, and that is exactly what happened.  Today, I am self-conscious enough to realize that there are people who would harshly judge me for the fact that I do come to know and understand the changing of the seasons – such a timeless and universal force of ‘nature’ that humanity is always subjected to – in part through a multi-national corporation’s advertising campaign.  So, fearing context collapse, I don’t publish those same kinds of photos and have those same kinds of interactions in the same place as I publish my academic musings.&lt;/p&gt;

&lt;p&gt;Yet the important thing to realize is that in posting these instagram photos of pumpkin spice lattes, I am likely contributing to some grand archive of humanity’s historical record – or at least there are people who think I am, which is probably even more important for this argument.  In fact, there are uncountably many digital artifacts on the Internet documenting the excitement leading up to everything from the McRib coming back to a new season of Mad Men premiering.  These are the kinds of interactions which are being recorded and increasingly preserved at a startling rate, compared to what kinds of materials we have typically chosen to preserve.  If we as a society preserve them not like members of previous generations individually preserved letters and memorabilia, but instead stored these interactions in massively-indexed digital archives, they will likely be an irresistible resource for future generations of historically-minded humanists and social scientists.  Perhaps this is where the tension lies: it could be that many people don’t want the records we leave for posterity to be filled with what is certainly not a representative sample of our collective cultural experience.  I somewhat agree with this sentiment, because I know that the people who post the most on these sites are probably some of the least representative of humanity.&lt;/p&gt;

&lt;p&gt;However, I must argue that if a future historian (or a contemporary social scientist or humanist) wants to seriously delve into what it is like for a certain segment of the population to be human and experience the world in 2012, they have to understand that they &lt;em&gt;ought&lt;/em&gt; to be looking a lot of nearly-identical photos of Starbucks products.  Not because pumpkin spice lattes themselves are such a culturally important phenomenon which reveal so much about the human condition – that’s completely the wrong way of looking at this.  Rather, the activity of sharing nostalgia-filtered instagram photos of the first pumpkin spice latte of the year is one way in which some members of a globalized, corporate consumer culture collectively experience the changing of the seasons.  If you’re not a part of a social group that engages in these kinds of practices, then you probably see the stray instagram photo that someone publishes to their Twitter stream as, well, something to be ridiculed.  You also may think that someone who has let a multi-million dollar corporate advertising campaign overcode their experience of nature is also independently deserving of ridicule, which I also disagree with, but that’s another issue entirely.&lt;/p&gt;

&lt;p&gt;On a side note, this ‘photography as documentation versus experience’ issue may also be why instagram, with all its filters and frames, gets so much hate. If you’re a photographic realist and understand photo sharing as a way of documenting the present world for an other who is not present in time and/or space, then those silly filters and frames seriously invalidate a core assumption behind such a practice.  However, if you instead understand photo sharing as a mode of communication in which we seek to not so much objectively document the external world &lt;em&gt;for others&lt;/em&gt; as subjectively express our experience _with others, _then filters and frames are probably one of the most innovative ‘features’ added to the social activity that is photography since the caption.&lt;/p&gt;

&lt;p&gt;This is also where I disagree with the critiques of photography from theorists like Barthes and Sontag, or more accurately, I think their critiques are only specific to the kinds of photo sharing practices which were prevalent in their time.  A photojournalist who waits for days to take an unrepresentative snapshot of a war zone is doing a completely different kind of ‘manipulation’ than someone who adds a washed-out filter to a smartphone photo of an empty street so that it more accurately conveys the dreariness they feel.  Sure, I’ll be the first to admit that instagram filters are also so prevalent because they enforce an aesthetic field in which almost any photo – even those blurry, overexposed shots quickly taken in poor lighting with crappy smartphone cameras – can be made to look “good.”  But that only strengthens my point: “serious” photographers who see instagram as a platform for collectively engaging in a centuries-old craft in which the world is captured onto a fixed medium don’t get that it is actually a platform for collectively engaging in a much older craft: conversation and storytelling.  In fact, I see these critiques as essentially the same ones Plato had of writing and rhetoric: How dare you make it easier for people to competently relate their experiences in a way that has meaning to themselves and the people around them!?!&lt;/p&gt;

&lt;p&gt;Those who study youth and social networking practices should already know that this entire issue is one of context collapse, but it is a more expanded case than the standard media narrative about college students posting wild photos that their parents or potential employers can see.  The issue is usually framed as stemming from the need to use the same platform to interact with multiple, overlapping, simultaneously-existing social worlds that hold different values about what is acceptable behavior and what is not.  However, I think that both of these cases also arise from a much less-discussed disagreement regarding the way in which participation in social networking sites is understood: When I share a photo of a party I attend, am I objectively documenting an event that happened to me, recording what took place so that my social network – including those people who I later friend on Facebook – can go through my profile and see how I’ve always been a cool party-goer?  Or am I sharing the photo to the people I currently interact with on Facebook, communicating that I was just at a fun party not as something that will stand on its own for all time, but instead something to serve as the basis for a conversation?  Either way, I will have to deal with the standard context collapse issues about how I should act in a social space where people from different social worlds are watching me, but this distinction is something more than that.&lt;/p&gt;

&lt;p&gt;This issue about the profile as a performance of the self versus the profile as a by-product of interactions seems to be my main frustration with something like Facebook’s now-mandatory Timeline feature.  Tensions over the rollout of Timeline, which aggregates your entire past on Facebook in an easy-to-read summary of your life, seem to be part of a larger trend that seeks to conflate these two understandings of what it means to engage in social activity online.  And as a side note, it is interesting that Timeline conflates this distinction with code, as opposed to cultural critics who conflate this with discourse.&lt;/p&gt;

&lt;p&gt;Anyways, after a terrible context collapse incident as a freshman in college, I like to think that I’ve always been a savvy Facebook user, self-censoring when I’m interacting in a space that could in any way be public.  Still, I just spent quite a long time trying to remove as much as I could from my Timeline, not because it contains anything that I would be seriously embarrassed about, but because it doesn’t represent who I am now in any way.  The people who I was friends with in 2005 aren’t the same as the people who I’m friends with in 2012, the things that mattered to me aren’t the same, the photos of me look nothing like I do now, and so on.&lt;/p&gt;

&lt;p&gt;Especially because there weren’t that many ways of interacting as there are now, since 2005 I have understood and used my Facebook profile as a carefully-curated representation of myself, working hard to remove those little interactions about how awesome last night was after they served their immediate communicative purposes.  However, that is getting harder and harder to do, which is why I move to other platforms – partially because their code is written in such a way that does not essentialize my interactions to form my profile, but also because the people who I communicate with share my same understanding of what it means to interact in such a space.&lt;/p&gt;</content><author><name>stuart</name></author><category term="communication" /><category term="community" /><category term="context collapse" /><category term="discourse" /><category term="facebook" /><category term="instagram" /><category term="internet" /><category term="photography" /><category term="pumpkin spice lattes" /><category term="social media" /><category term="timeline" /><summary>I don’t normally pick on people whose work I really admire, but I recently saw a tweet from Mark Sample that struck a nerve: “Look, if you don’t instagram your first pumpkin spice latte of the season, humanity’s historical record will be dangerously impoverished.”  While it got quite a number of retweets and equally snarky responses, he is far from the first to make such a flippant critique of the vapid nature of social media.  It also seriously upset me for reasons that I’ve been trying to work out, which is why I found myself doing one of those shifts that researchers of knowledge production tend to do far too often with critics: don’t get mad, get reflexive.  What is it that makes such a sentiment resonate with us, particularly when it is issued over Twitter, a platform that is the target of this kind of critique?  The reasons have to do with a fundamental disagreement over what it means to interact in a mediated space: do we understand our posts, status updates, and shared photos as representations of how we exist in the world which collectively constitute a certain persistent performance of the self, or do we understand them a form of communication in which we subjectively and interactionally relate our experience of the world to others?</summary></entry><entry><title>The ethnography of robots: interview at Ethnography Matters</title><link href="http://stuartgeiger.com/posts/2012/08/the-ethnography-of-robots-interview-at-ethnography-matters/" rel="alternate" type="text/html" title="The ethnography of robots: interview at Ethnography Matters" /><published>2012-08-14T10:55:17-07:00</published><updated>2012-08-14T10:55:17-07:00</updated><id>http://stuartgeiger.com/posts/2012/08/the-ethnography-of-robots-interview-at-ethnography-matters</id><content type="html" xml:base="http://stuartgeiger.com/posts/2012/08/the-ethnography-of-robots-interview-at-ethnography-matters/">&lt;p&gt;This was an interview I did with the wonderful &lt;a href=&quot;http://hblog.org&quot;&gt;Heather Ford&lt;/a&gt;, &lt;a href=&quot;http://ethnographymatters.net/2012/01/15/the-ethnography-of-robots/&quot;&gt;originally posted&lt;/a&gt; at &lt;a href=&quot;http://www.ethnographymatters.com&quot;&gt;Ethnography Matters&lt;/a&gt; (a really cool group blog) way back in January. No idea why I didn’t post a copy of this here back then, but now that I’m moving towards my dissertation I’m thinking about this kind of stuff more and more.  In short, I argue for a non-anthropocentric yet still phenomenological ethnography of technology, studying not the culture of the people who build and program robots, but the culture of those the robots themselves.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;em&gt;Heather Ford spoke with Stuart Geiger, PhD student at the UC Berkeley School of Information, about his emerging ideas about the ethnography of robots. “Not the ethnography of robotics (e.g. examining the humans who design, build, program, and otherwise interact with robots, which I and others have been doing),” wrote Geiger, “but the ways in which bots themselves relate to the world”. Geiger believes that constructing and relating an emic account of the non-human should be the ultimate challenge for ethnography but that he’s getting an absurd amount of pushback from it.” He explains why in this fascinating account of what it means to study the culture of robots.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;HF: So, what’s new, almost-Professor Geiger?&lt;/p&gt;

&lt;p&gt;SG: I just got back from the 4S conference — the annual meeting of the Society for the Social Study of Science — which is pretty much the longstanding home for not just science studies but also Science and Technology Studies. I was in this really interesting session featuring some really cool qualitative studies of robots, including two ethnographies of robotics. One of the presenters, Zara Mirmalek, was looking at the interactions between humans and robots within a modified framework from intercultural communication and workplace studies.&lt;/p&gt;

&lt;p&gt;I really enjoyed how she was examining robots as co-workers from different cultures, but it seems like most people in the room didn’t fully get it, thinking it was some kind of stretched metaphor. People kept giving her the same feedback that I’ve been given — isn’t there an easier way you can study the phenomena that interest you without attributing culture to robots themselves? But I saw where she was going and asked her about doing ethnographic studies of robot culture itself, instead of the culture of people who interact with robots — and it seemed like half the room gave a polite chuckle. Zara, however, told me that she loved the idea and we had a great chat afterwards about this.&lt;/p&gt;

&lt;p&gt;HF: What do you think people are upset about?&lt;/p&gt;

&lt;p&gt;SG: The more middle-of-the-road stances come from people who don’t personally have a strong reaction either way, but tell me that I’ll have to fight an uphill battle from angry humanists who I’ll talk about later. These people aren’t really against the idea, but they don’t really see the value added in ascribing culture to the non-humans. They tell me that there are better and more non-controversial ways of analyzing, say, distributed cognition in a heterogeneous network of humans and robots. It’s a response that I appreciate, because it would be futile to have to go through all of this work on an ethnography of robots if my analysis is otherwise identical to an ethnography of robotics. And then the most polite responses I get are people who tell me it is interesting, and then when I prod them further to ask them if they actually buy it, they tell me that they don’t *yet* think it can be done, but would like to see what I end up with.&lt;/p&gt;

&lt;p&gt;Some of the really negative responses I get involve a visceral reaction against attributing ‘culture’ to the realm of the non-human. I understand this — anthropology is, by definition, anthropocentric: it is concerned with the human condition, as it is constituted in various localities and peoples. This is the same fight we Latourians have with sociologists about the term “agency”: there is a very deeply-rooted assumption that humans have some innate, unique qualities that distinguish us from not only mere matter but other animals as well. When someone comes along and makes a very nuanced point about how objects have agency, the most immediate and natural response is first of all anthropomorphism, which is easy to rebut.&lt;/p&gt;

&lt;p&gt;But then comes a much more worthy ontological argument from people who really know their stuff: that when Latour ascribes agency to objects, he actually manages to do so by keeping the agency of humans and the agency of non-humans symmetrical. Against the standard, boring objection that he ascribes too many human characteristics to non-humans, what is really going on is that he accomplishes so much by taking away so many of those ‘uniquely human’ qualities from human agents. This is why Latour never goes inside of anyone’s head, why he rarely tries to give a psychological or cognitive account in the actor-networks he studies. (Read Latour’s review of “Cognition in the Wild” by Ed Hutchins for more on this, and you can see that he loves the idea that these seemingly human abilities like cognition are not pre-given but themselves an effect of a heterogeneous network of humans and non-humans.)&lt;/p&gt;

&lt;p&gt;Anyways, far from being an anthropomorphism, Latour’s ontology is flat, in which all entities have the same capacities. That is, they have the same a priori capabilities, but they are definitely not equal after socio-technical relations emerge and start operating. This all means that against the vulgar interpretations of ANT, objects don’t have intentionality or consciousness, because — and this is the really important point — neither do humans. Or, in another interpretation, perhaps humans do have intentionality or consciousness, but it makes no difference one way or another. A good actor-network theorist is able to take some existing system in which there are far too many explanations based on those uniquely human qualities and give an alternative account that relies instead on materials, technologies, infrastructures, documentation, and other modes of externalized practices. It is not to make the more futile argument that norms and consciousness and all those warm fuzzy humanisms don’t exist, but that they’re not necessary.&lt;/p&gt;

&lt;p&gt;Anyways, the same thing happens with me in my ethnography of robots, as I’m effectively taking life out of culture. You can see why both sociologists and anthropologists object to this, albeit for slightly different reasons. Sociologists will allow, for example, some analysis of the sociality of bees, while anthropologists will reject out of hand an ethnography of bees (which like robots/robotics, is different from an ethnography of bees-with-humans). But both seem opposed to attributing sociality or culture to a fundamentally non-living set of individuals. Or even calling non-living entities ‘individuals’ in the first place. And I won’t fall into the trap of saying that robots are living and then mapping human categories onto robot phenomena (e.g. consciousness = statefulness, cognition = code), even though it might seem to make things easier in the short-term. More on that later, but for now be content that all of these things are possible without robots having some advanced AI.&lt;/p&gt;

&lt;p&gt;Any ethnography of a non-human society would have to fight this same kind of battle that Latour fought over agency, if it didn’t wish to succumb to the very tempting but misguided prospect of simply importing and mapping existing ontological categories from sociology: e.g. norms in a robot society are found in protocols. This, by the way, would then be using ‘culture’ and indeed the entire ethnographic framework as one massively-stretched analogy, which isn’t the point. The argument is not so much that a robot society is very much ‘alive’ in the same way that human societies have, say, deviant individuals, fluid norms, fascinating rituals, internal contradictions, complicated power relations, and many more weirdly beautiful and complex aspects hidden just below the surface.&lt;/p&gt;

&lt;p&gt;Rather, the point of anthropology is typically to locate a people who are typically strange and foreign to us, and then relate the way in which those people live, showing not only how they are different from us but also how they are the same. In doing so, we learn not only about others, but also ourselves. So in that framework, I tend to agree with the critics who say that only way to give a vitalistic account of a robot society is by projecting too many human qualities onto the non-human. What is then left is a non-vitalistic ethnography: an account of a culture devoid of life. Like with Latour and agency, once we show that life is not a necessary criterion for this thing called culture, then the fun really begins — and you can see why lots of people would oppose this.&lt;/p&gt;

&lt;p&gt;HF: No friends for robot anthropology, then?&lt;/p&gt;

&lt;p&gt;SG: I do have some allies and kindred spirits, and I keep returning to this quote from Deleuze and Guattari’s A Thousand Plateaus on music: “Of course, as Messiaen says, music is not the privilege of human beings: the universe, the cosmos, is made of refrains … The question is more what is not musical in human beings, and what is already musical in nature. Moreover, what Messiaen discovered in music is the same thing ethnologists discovered in animals: human beings are hardly at an advantage, except in the means of overcoding, of making punctual systems.” Music is but one of many domains that is typically seen as inherently social and therefore uniquely human, and the anthropocentric perspective tends to reduce everything to how it functions in the human experiential frame. And on a side note, this is why I’m so excited by Ian Bogost’s upcoming book “Alien Phenomenology: Or What It’s Like To Be A Thing” — the title just says it all, doesn’t it?&lt;/p&gt;

&lt;p&gt;And before you start to think that I’m envisioning some sort of AI-based fantasy of the singularity in which robots start to replace all of us social humans — therefore locating the sociality of robot culture in its ability to stand in for humans — that’s definitely the exact opposite of where I’m going. Robots can be said to have their own culture precisely because they don’t need to copy our sociologisms in order to be social, although what they do in their own social realm may not easily map on to things we do in our social realm. This is probably what fascinates me most about this project. And it is precisely for this reason that we must absolutely resist the temptation to make cheap analogies between things that happen in robot culture and human culture, such as saying that protocols are just robot norms.&lt;/p&gt;</content><author><name>stuart</name></author><category term="actor-network theory" /><category term="ANT" /><category term="bots" /><category term="ethnography" /><category term="latour" /><category term="network" /><category term="technology" /><summary>This was an interview I did with the wonderful Heather Ford, originally posted at Ethnography Matters (a really cool group blog) way back in January. No idea why I didn’t post a copy of this here back then, but now that I’m moving towards my dissertation I’m thinking about this kind of stuff more and more.  In short, I argue for a non-anthropocentric yet still phenomenological ethnography of technology, studying not the culture of the people who build and program robots, but the culture of those the robots themselves.</summary></entry><entry><title>Closed-source papers on open source communities: a problem and a partial solution</title><link href="http://stuartgeiger.com/posts/2011/06/closed-source-papers-on-open-source-communities-a-problem-and-a-partial-solution/" rel="alternate" type="text/html" title="Closed-source papers on open source communities: a problem and a partial solution" /><published>2011-06-12T04:24:27-07:00</published><updated>2011-06-12T04:24:27-07:00</updated><id>http://stuartgeiger.com/posts/2011/06/closed-source-papers-on-open-source-communities-a-problem-and-a-partial-solution</id><content type="html" xml:base="http://stuartgeiger.com/posts/2011/06/closed-source-papers-on-open-source-communities-a-problem-and-a-partial-solution/">&lt;p&gt;In the Wikipedia research community — that is, the group of academics &lt;em&gt;and Wikipedians&lt;/em&gt; who are interested in studying Wikipedia — there has been a pretty substantial and longstanding problem with how research is published. Academics, from graduate students to tenured faculty, are deeply invested and entrenched in an system that rewards the publication of research. Publish or perish, as we’ve all heard.   The problem is that the overwhelming majority of publications which are recognized as ‘academic’ require us to assign copyright to the publication, so that the publisher can then charge for access to the article.  This is in direct contradiction with the goals of Wikipedia, as well as many other open source and open content creation communities — communities which are the subject of a substantial amount of academic research.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;strong&gt;Freely-accessible or freely-licensed?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are actually two issues here, the first being that members of these communities want access to research about themselves without having to pay the average $20-$30 an article.  While important, this also overshadows a more fundamental concern: communities like Wikipedia, Apache, Creative Commons, and OLPC were founded on the idea of providing free and open software, hardware, or educational content to the world.   The &lt;a href=&quot;http://wikimediafoundation.org/wiki/Mission_statement&quot;&gt;Wikimedia Foundation’s mission statement&lt;/a&gt; is “to empower and engage people around the world to collect and develop educational content under a &lt;a href=&quot;http://en.wikipedia.org/wiki/en:free_content&quot; title=&quot;w:en:free content&quot;&gt;free license&lt;/a&gt; or in the public domain.”  That is pretty clear-cut, and those of us with obligations to both our own academic community and the Wikipedia community are having more and more problems with negotiating those competing tensions.&lt;/p&gt;

&lt;p&gt;In a sense, this is related to how the major ethical dilemma with 19th and early 20th century anthropologists wasn’t about giving ‘their natives’ a copy of their manuscripts. Rather, it was that most anthropologists were participating in systems of colonialism, which were in direct opposition to the interests of the people they studied.  Now, I am in no way arguing that the same kind of power relation exists between academics who study Wikipedians and the Wikipedian community, or that the issue open educational sources is on the same ethical level as colonialism.   As an aside, contemporary anthropologists have documented this shift from ‘studying down’ to ‘studying up’, although I would say that most academics who research open communities like Wikipedia are now ‘studying across’ — but that interesting subject is for another blog post.   But I bring it up because unlike with the Trobriand Islanders, the communities that we study are now beginning to articulate their concerns with how we perform and publish our research, and it is something that we need to listen to.&lt;/p&gt;

&lt;p&gt;So to return to the core issue at hand: why is the Wikipedian community (and the Wikimedia Foundation) supporting research that will be copyrighted and bound up in publications which further support an intellectual property regime they clearly stand against?   And what does it mean for us as academic researchers to give back to the communities we study?   It obviously goes beyond being willing to send a copy of a PDF to an interested Wikipedian over e-mail, or even hosting a freely-accessible copy of our copyrighted PDFs on our websites (which many of us do, even when we’re not supposed to). For those of us studying Wikipedia, Creative Commons, Scratch, or a number of open content creation communities, it means releasing our research under &lt;a href=&quot;http://creativecommons.org/licenses/&quot;&gt;a Creative Commons license&lt;/a&gt;, as this has become the standard for releasing everything other than code.&lt;/p&gt;

&lt;p&gt;Now, the moment I say this, all the academics breathe a heavy sigh, knowing that such a request is impossible, given the current academic system in which we are entrenched. Even the &lt;a href=&quot;http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1083-6101&quot;&gt;Journal of Computer Mediated Communication&lt;/a&gt;, one of the few top-tier open access journals in the social sciences, is copyrighted by the publisher. Some academic superstars like Lawrence Lessig have been able to get their books published from a university press while still being released under a CC license, but not all of us are Lawrence Lessig. Especially for graduate students and junior faculty, who are desperately trying to get their research published anywhere, when the paper finally gets accepted and that copyright assignment form comes in your inbox, the last thing you want to do is start a losing battle over CC-BY-SAing your paper. However, I do have to give a shoutout to Joseph Reagle, who spent a massive amount of effort getting MIT Press to let him publish &lt;a href=&quot;http://reagle.org/joseph/2010/gfc/&quot;&gt;his book on Wikipedia&lt;/a&gt; under a CC license (although with a number of restrictions), but it is unclear the extent to which this will continue in the future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A partial solution: freely-licensed figures, ‘used with permission’ in copyrighted research papers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So now I finally get to the solution that this blog post was supposed to be entirely about. We academics who study open content communities have an obligation to release our research under free licenses. This does not mean that we have to release our &lt;em&gt;research papers&lt;/em&gt; under &lt;a href=&quot;http://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;CC-BY-SA&lt;/a&gt;, which is all but impossible for most of us. What it means is that we must release our findings, results, and conclusions under such licenses, and thanks to how copyright works, we can do this through the existing system. Conclusions and abstracts are easy: we just re-write them. We should actually be in the habit of re-writing our densely-worded abstracts and conclusions under a more succinct and human-readable for the communities we study anyway.&lt;/p&gt;

&lt;p&gt;However, there is also a way to do this with figures, charts, and graphs. This idea came to me when I saw a copyrighted article in the ACM library (from the Association for Computing Machinery, where a significant amount of Wikipedia research is published) which used a photo someone else took “with permission.” This kind of thing happens regularly enough for the ACM to have &lt;a href=&quot;http://www.acm.org/publications/policies/copyright_policy&quot;&gt;a rather sane policy&lt;/a&gt; on it: “The author’s copyright transfer applies only to the work as a whole, and not to any embedded objects owned by third parties. An author who embeds an object, such as an art image that is copyrighted by a third party, must obtain that party’s permission to include the object, with the understanding that the entire work may be distributed as a unit in any medium.” I haven’t checked any other publication houses, but I’ve seen this kind of situation happen in so many different books and papers that it could provide a nice loophole in for most of academia.&lt;/p&gt;

&lt;p&gt;For most research on Wikipedia, the figures, charts, and graphs are the most interesting aspects of the research, and these can be released under a &lt;a href=&quot;http://creativecommons.org/licenses/by/3.0/&quot;&gt;CC-BY&lt;/a&gt; or &lt;a href=&quot;http://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;CC-BY-SA&lt;/a&gt; license, and then used with permission in an ACM article. The ACM’s main concern is that they need authors to assign copyright to them in order to make sure publication goes smoothly, and as long as the ‘original author’ of the image is completely fine with having the image in the work and published by the ACM, everyone is happy. I’m no lawyer, but I think this would work with releasing figures, charts, and graphs, even though the copyright policy only qualifies the legal phrase with an example of art images copyrighted by third parties. This doesn’t work as well with many forms of qualitative research, such as historical or interview-based research in which the goal is to elaborate on specific case studies. Still, figures and conceptual diagrams are also useful in those kinds of papers, and can be added to an alternative documentation of a research project, which is possibly co-extensive with &lt;em&gt;but not identical to&lt;/em&gt; the research paper.&lt;/p&gt;

&lt;p&gt;I’ve actually been putting my charts and graphs up on &lt;a href=&quot;http://commons.wikimedia.org&quot;&gt;Wikimedia Commons&lt;/a&gt; for quite some time (you can check them all out on &lt;a href=&quot;http://commons.wikimedia.org/wiki/Special:ListFiles/Staeiou&quot;&gt;my user gallery&lt;/a&gt;), even before I realized that copyright was even an issue.   These figures are present in my published papers, many of which are copyrighted by the ACM.  Thankfully, it turns out that this is actually compatible copyright-wise, but this is only solid because I uploaded them to Commons before assigning copyright to the ACM.  It is less clear if someone can retroactively release such images.&lt;/p&gt;

&lt;p&gt;But that issue aside, my graphs and charts can live in both worlds, serving members of both communities.  For my quantitative research, these graphs contain my core findings about the rise of bots and assisted editing tools, for example. I have yet to document my previous research projects in a way that would be helpful to others.  More on that in the section below, but I think that even just uploading figures to Commons is a good start.  And it is incredibly painless, especially given that uploading to Commons is a lot easier now than it has been in the past.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Research documentation on Meta-Wiki&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;**&lt;/em&gt;Documentation of research projects could take place quite nicely in &lt;a href=&quot;http://meta.wikimedia.org/wiki/Research:Projects&quot;&gt;a new Research: namespace&lt;/a&gt; that some great people at the Wikimedia Foundation have provided to document planed, current, and past research projects on Meta-Wiki, the wiki that is used to coordinate many tasks which are common to all language versions of Wikipedia, as well as projects like Wikisource or Wiktonary. You can see a very rough example of one of these that I am working on with as part of my summer research  fellowship with the Wikimedia Foundation: &lt;a href=&quot;http://meta.wikimedia.org/wiki/Research:Alternative_lifecycles_of_new_users&quot;&gt;an incomplete but still interesting study of new users&lt;/a&gt; that fellow-Fellow Jonathan Morgan and I are doing.&lt;/p&gt;

&lt;p&gt;The documentation page is not written like an academic article, although it does give Wikipedians and researchers alike something that is arguably more important. It gives information necessary to replicate the study, for example, how we sampled for new users and what coding schema we used to track new user participation in community spaces. It also contains a few sentences about the motivation of the study, and a few sentences about each of the results. And critically, it contains the graphs which clearly indicate that since 2004, fewer users are participating in community spaces in their first thirty days of joining the project.  If I wanted to write this up into an academic article (which I do plan to), I can do so in such a way that is both suitable for the ACM or another academic publisher, while keeping all the existing content on the documentation page freely-licensed.&lt;/p&gt;

&lt;p&gt;Now, to be on the safe side, it may be wise to release these graphs under a &lt;a href=&quot;http://creativecommons.org/licenses/by/3.0/&quot;&gt;CC-BY&lt;/a&gt; license instead of a &lt;a href=&quot;http://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;CC-BY-SA&lt;/a&gt; one, because the &lt;a href=&quot;http://en.wikipedia.org/wiki/Share-alike&quot;&gt;Share Alike&lt;/a&gt; requirement might require some other researcher to release an entire academic paper under a CC-BY-SA license if they use one of my CC-BY-SA figures.   However, I do not think this is the case, because as I am the original copyright holder, I can choose to give permission to using images in my own academic papers.   This is a common misconception with Share Alike and CC licenses in general — while I can never revoke my license once I make it, I am not bound by those terms in my own work, and can release the image under as many free and non-free licenses as I choose.   For example, if it is entirely my own image that I license with CC-BY-SA, I do not have to release every work that builds on it under CC-BY-SA, just as I can license the work for commercial use even if I choose a CC license that prohibits commercial use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Research isn’t a paper&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In all, I think that many of the seemingly-intractable problems stem from the false assumption that research projects are entirely encapsulated in a series of papers, and so the demand to ‘freely license your research’ is heard as ‘freely license your papers’.  However, academics already think of research projects as these long processes which spawn multiple papers, and so there is no reason why a research project could not also spawn a freely-licensed documentation space which does not prohibit the publishing of research papers.  Certainly there are many aspects of research papers which would not be included, and there is a risk that these documentation spaces would be second-class reports which are always incomplete compared to the research paper.  Though it is a bit patronizing to universally assume that community members don’t want that dense theoretical analysis of how distributed cognition flows in the actor-network, I think that a facts, figures, and abstracts version would suffice for most.&lt;/p&gt;

&lt;p&gt;Given the current academic systems in which we are currently entrenched, I think that this is a good short-term solution, especially for graduate students and other junior scholars who do not have the political capital to change the way in which existing publication regimes operate.  And who knows, perhaps by creating alternative, freely-licensed spaces for documenting research, these publications will recognize the need to make research, though not necessarily research papers, freely accessible and open to all.&lt;/p&gt;</content><author><name>stuart</name></author><category term="academia" /><category term="copyright" /><category term="creative commons" /><category term="education" /><category term="intellectual property" /><category term="open access" /><category term="research" /><category term="wikimedia foundation" /><category term="wikipedia" /><summary>In the Wikipedia research community — that is, the group of academics and Wikipedians who are interested in studying Wikipedia — there has been a pretty substantial and longstanding problem with how research is published. Academics, from graduate students to tenured faculty, are deeply invested and entrenched in an system that rewards the publication of research. Publish or perish, as we’ve all heard.   The problem is that the overwhelming majority of publications which are recognized as ‘academic’ require us to assign copyright to the publication, so that the publisher can then charge for access to the article.  This is in direct contradiction with the goals of Wikipedia, as well as many other open source and open content creation communities — communities which are the subject of a substantial amount of academic research.</summary></entry></feed>
